{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Install Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the packages to run the training and testing successfully as follows:\n",
    "- `diffusers`, Reference: https://github.com/huggingface/diffusers, https://huggingface.co/docs/diffusers/en/index\n",
    "- `kohya_ss`, Reference: https://github.com/bmaltais/kohya_ss\n",
    "- `xformers` and Libs in `requirements.txt` (modified from `kohya_ss` package)\n",
    "\n",
    "**Step 1: Create and switch to a new virtual environment in this directory for installation (Recommended).**\n",
    "   - Install `Anaconda` or `Miniconda`.\n",
    "   - Run the command: `conda create --prefix ./env python=3.10`\n",
    "   - After the environment is created, choose `env` as the working kernel.\n",
    "   \n",
    "**Step 2: Install and set up all required libraries and documents.**\n",
    "   - If you encounter an initialization problem, `switch the kernel to another environment, and then switch it back to the newly created one`.\n",
    "   - Restart the kernel after installation.\n",
    "   - You can comment back on the code lines after installation to not repeat the installation process.\n",
    "\n",
    "**Notice: If you cannot successfully run the installation process, please manually `git clone` the `diffusers` and `kohya_ss` repos to this project's `Root Directory` and follow the command lines for further usage.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-1: Huggingface Diffusers package**\n",
    "This is the official package of `Hugging Face Diffusers`, which contains many useful built-in functions to build our own pipelines for diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/diffusers\n",
    "!cd diffusers\n",
    "%pip install .\n",
    "!cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-2: Kohya_ss package**\n",
    "This is the official package of `kohya_ss`, which is currently a well known training package for Stable Diffusion models, you can either setup and use its UI, or use the pipelines provided in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recurse-submodules https://github.com/bmaltais/kohya_ss.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-3: PyTorch and CUDA**\n",
    "Please download the latest stable version of `PyTorch` and `CUDA` for your hardware system (https://pytorch.org/get-started/locally), the code here is for `Windows OS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-4: Other libraries and installation**\n",
    "The `xformers` library will massively increase performance and improve memory usage during training and testing.\n",
    "\n",
    "Please also notice that running diffusion models requires a high level of hardware systems, especially GPU.\n",
    "\n",
    "This project was run with `CPU: AMD Ryzen 7 7800X3D 8-Core Processor` and `GPU: NVIDIA GeForce RTX 4070 / 12g VRAM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install xformers\n",
    "%pip install -U peft\n",
    "%pip install omegaconf\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice: Please restart the kernel and run again if everything is installed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-4: Customize accelerate document**\n",
    "Please run the default command to create a file called `default_config.yaml` in your cache folder. It is normally located at `~/.cache/huggingface/accelerate`,\n",
    "\n",
    "or `your environment variable HF_HOME` suffixed with `accelerate`, or `your environment variable XDG_CACHE_HOME` suffixed with `huggingface/accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !accelerate config default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please copy and paste these configs (without the comments) to overwrite the `default_config.yaml` file in your cache folder.\n",
    "\n",
    "Feel free to modify `existing items` if your hardware is different from the settings, but do not add `additional items` for the provided template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified setting for default_config.yaml:\n",
    "\n",
    "## compute_environment: LOCAL_MACHINE\n",
    "## debug: false\n",
    "## distributed_type: 'NO'\n",
    "## downcast_bf16: 'no'\n",
    "## gpu_ids: all\n",
    "## machine_rank: 0\n",
    "## main_training_function: main\n",
    "## mixed_precision: fp16\n",
    "## num_machines: 1\n",
    "## num_processes: 1\n",
    "## rdzv_backend: static\n",
    "## same_network: true\n",
    "## tpu_env: []\n",
    "## tpu_use_cluster: false\n",
    "## tpu_use_sudo: false\n",
    "## use_cpu: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! If you are done with the setup, then we are good to do for the next step.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Dataset and Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided dataset contains 20 selected pictures `(.jpg)` and the corresponding caption `(.txt)` of `Geralt of Rivia`, who is a very well-known character from the series `The Witcher`.\n",
    "\n",
    "The pictures were mostly sampled from the game `The Witcher 3: Wild Hunt` which was first released in 2015 and is still the latest episode of the story. The process of data collecting is as follows:\n",
    "\n",
    "**Step 1: Search and download gaming pictures.**\n",
    "   - Searched from `Wallhaven` (https://wallhaven.cc) which has lots of pictures with good quality.\n",
    "   - Selected and downloaded 35 pictures with different features of `Geralt`, e.g. his face, only upper body, full body, different poses, different angles, etc.\n",
    "   - Carefully picked the 20 pictures that contain 7 faces, 10 upper-body, and 3 full-body pics for the dataset creation.\n",
    "\n",
    "**Step 2: Pre-processing for the image data**\n",
    "   - Used the tool from `Birme` (https://www.birme.net) to crop, resize, and transform the image format in batches.\n",
    "   - Cropped the unrelated elements, characters, and complicated objects, and let `Geralt` be the main part of each picture.\n",
    "\n",
    "**Step 3: Pre-processing for the caption data**\n",
    "   - Used the `wd14-tagger` plugin on `webUI` (which is a well-known interface to implement various tasks with Stable Diffusion models) to mark and auto-generate captions for each picture.\n",
    "   - Manually added additional captions and deleted inappropriate generated captions for all 20 pictures.\n",
    "   - It is important to know here that we set the `trigger word` for our LoRA to be `geralt of rivia`, which means we expect to see a strong effect of LoRA when entering `trigger word` as part of the prompt.\n",
    "   - Reference: `wd14-tagger` (https://github.com/picobyte/stable-diffusion-webui-wd14-tagger), `webUI` (https://github.com/AUTOMATIC1111/stable-diffusion-webui).\n",
    "\n",
    "**Step 4: Place the dataset in the folder and finalize the pre-processing**\n",
    "   - Created folder (called `100_geralt of rivia man`, which is already provided in this project).\n",
    "   - Place all `.jpg` and `.txt` data in this folder as the dataset for LoRA training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-1: Import dependencies**\n",
    "We import the following dependencies for this project, including PyTorch, Tensorboard, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorboard import notebook\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from huggingface_hub import snapshot_download\n",
    "from xformers.ops import MemoryEfficientAttentionFlashAttentionOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-2: Show pre-processed dataset**\n",
    "We show 6 randomly selected pictures as a demo to help you understand what the dataset actually looks like, including the `pictures of Geralt` and the `corresponding captions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image and text file paths\n",
    "img_indices = [1, 5, 11, 13, 16, 20]\n",
    "img_files = [f\"./dataset/img/100_geralt of rivia man/{i}.jpg\" for i in img_indices]\n",
    "txt_files = [f\"./dataset/img/100_geralt of rivia man/{i}.txt\" for i in img_indices]\n",
    "\n",
    "# Create a figure with 3 x 2 subplots\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 18))  # Increase the figure size\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        # Calculate the index of the image and text file\n",
    "        index = i * 2 + j\n",
    "\n",
    "        # Read the image file\n",
    "        img_data = mpimg.imread(img_files[index])\n",
    "\n",
    "        # Read the text file\n",
    "        with open(txt_files[index], 'r') as file:\n",
    "            prompt = file.read()\n",
    "\n",
    "        # Display the image\n",
    "        axs[i, j].imshow(img_data)\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "\n",
    "        # Display the prompt below the image\n",
    "        axs[i, j].text(0.5, -0.25, prompt, wrap=True, horizontalalignment='center', fontsize=12, transform=axs[i, j].transAxes)\n",
    "\n",
    "plt.subplots_adjust(wspace=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Training Process and Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few points you would like to know before starting this part: \n",
    "\n",
    "- The purpose of this project is to `prepare a whole dataset from scratch` and to train `LoRA adapters` to learn a specific character, who is `Geral of Rivia` here.\n",
    "\n",
    "- We then test how powerful the `LoRA technique` is for `learning a specific character (or specific style)`.\n",
    "\n",
    "- Instead of using the latest `SDXL` model, we trained the LoRA adapter on `Stable Diffusion v1.5` with a more stable and reliable training process.\n",
    "\n",
    "Let's dive deeper into the training process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-1: Open Tensorboard (re-run the code here if you want to monitor results)**\n",
    "We first open `Tensorboard` to monitor the LoRA training process (if you would like to run it), and the results from the log files, which are located at `./output/log/`.\n",
    "\n",
    "Since the training process would be long, and it takes around 40 minutes (on my hardware system):\n",
    "\n",
    "- If you don't prefer to run it, this project has attached the trained model and its `log file`, you can simply open `Tensorboard` to look at the given training records!\n",
    "\n",
    "- This `.ipynb` file was also executed with a clean run with all the results for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a Tensorboard instance\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir={\"./output/log\"} --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List current Tensorboard instances\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Tensorboard (Tensorboard will display here)\n",
    "notebook.display(port=6006, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-2: Close Tensorboard**\n",
    "Please comment the code if you do not want to automatically close the `Tensorboard`.\n",
    "\n",
    "Leaving this code uncomment can avoid initialization problems during the first run before restarting the kernel since `Tensorboard will not close with the restart`.\n",
    "\n",
    "To open the `Tensorboard` again, please re-run the code blocks at `3-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close existing TensorBoard instances\n",
    "!taskkill /IM \"tensorboard.exe\" /F\n",
    "!rmdir /S /Q %temp%\\.tensorboard-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3-3: Run training script**\n",
    "The main idea of LoRA training is to freeze the base model's weights and insert trainable layers, which are matrices for rank decomposition, in each transformer block. We do not need to train all weights of the base model while using the LoRA training technique since all weights can be divided and stored as matrices with far fewer weights to train in total.\n",
    "\n",
    "We selected and used the existing training framework of standard LoRA training for Stable Diffusion v1.5 provided in the `kohya_ss` library, this script file is called `train_network.py` and located at `./kohya_ss/sd-scripts/train_network.py`, while we customized the parameters for this project's training process.\n",
    "\n",
    "Some important training parameters are as below:\n",
    "\n",
    "1. `pretrained_model_name_or_path`: Select `Stable Diffusion v1.5` as the base model of the series that we want the LoRA adapter to apply to.\n",
    "\n",
    "2. `mixed_precision`: Select `fp16` to implement half-precision data types during the training, this can reduce the memory usage and increase the performance.\n",
    "\n",
    "3. `resolution`: Select `512 x 512`, since the base model `Stable Diffusion v1.5` was trained with this size of images, so we want to keep Consistency with our dataset.\n",
    "\n",
    "4. `seed`: Set the seed number to `reduce the randomness` when we want to reproduce a certain image result (but randomness still exists since the random crop still exists).\n",
    "\n",
    "5. `cache_latents`: Turn on cache latents to compress the images to smaller latents stored in VRAM as cache to speed up the training process.\n",
    "\n",
    "6. `unet_lr`: Remained as default value `0.0001` for LoRA training with the U-Net learning rate on the inference process (denoising and feature extraction).\n",
    "\n",
    "7. `text_encoder_lr`: Remained as default value `0.00005` for LoRA training with the text encoder learning rate (text encoder is the CLIP model `CLIP ViT-L/14` in `SD v1.5`).\n",
    "\n",
    "8. `loss_type`: Select `L2 loss` (MSE loss) since we want to let the model be less sensitive to small variances but very responsive to reducing large errors, this can be important for noise reduction.\n",
    "\n",
    "9. `lr_scheduler`, `lr_scheduler_num_cycles`,`lr_warmup_steps`: This is for adjusting the learning rate to prevent overfitting and underfitting cases during the process, we select the scheduler `cosine_with_restarts` for cyclical change of the learning rate with `10%` of the total steps as warm-up steps (`400 steps`) and `4` period of cycles during the training.\n",
    "\n",
    "10. `train_batch_size`: Set to `2` for processing 2 images in the same batch.\n",
    "\n",
    "11. `max_train_steps`: Entered `4000` steps, which is also determining the total epoch. This is because the training script reads the dataset folder name and takes the prefix `100` as the steps for each image, we have 20 images and the batch size is set to 2, so `100 x 20 / 2 = 1000` we have 1000 steps per epoch. Since the maximum number of steps is `4000`, we end up with `4` training epochs in total.\n",
    "\n",
    "12. `save_every_n_epochs`: Set to `1` because we want to have middle LoRA checkpoints for each epoch, and then compare them in the testing part.\n",
    "\n",
    "13. `network_dim`: The dimension of LoRA network, we select a higher value to let the LoRA adapter learn as many features as possible, and since we filtered the pictures, unrelated elements are fewer in our dataset.\n",
    "\n",
    "14. `network_alpha`: Alpha is a parameter for the regularization term (penalty term), we set it to a high value since we only provide 20 pictures and want to prevent overfitting (expect the decision boundary to have lesser curvatures), this is because smaller weights tend to fix high variance when increasing alpha. \n",
    "\n",
    "15. `optimizer_type`: Selected `AdamW8bit` as the optimizer, since it can decouple weight decay from the gradient updates, then apply directly to the weights. In addition, it can also reduce the bit width from 32-bit float points to 8-bit integers, resulting in better stability and faster training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is the code to start the training script, you can choose to skip this part, and go to the next code block to download my trained LoRA checkpoints.**\n",
    "\n",
    "**Uncomment the code if you would like to try running it!**\n",
    "\n",
    "**Notice: The process may take some time to complete and depends on the hardware used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please uncomment the following code to train the model\n",
    "'''\n",
    "!accelerate launch --mixed_precision=\"fp16\" --num_processes=1 --num_machines=1 --num_cpu_threads_per_process=2 \".\\kohya_ss/sd-scripts/train_network.py\" \\\n",
    "--pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
    "--train_data_dir=\"./dataset/img\" \\\n",
    "--output_name=\"lora-train\" \\\n",
    "--output_dir=\"./output/model\" \\\n",
    "--logging_dir=\"./output/log\" \\\n",
    "--resolution=\"512,512\" \\\n",
    "--caption_extension=\".txt\" \\\n",
    "--seed=\"1234\" \\\n",
    "--mixed_precision=\"fp16\" \\\n",
    "--bucket_no_upscale \\\n",
    "--bucket_reso_steps=64 \\\n",
    "--cache_latents \\\n",
    "--gradient_checkpointing \\\n",
    "--huber_c=\"0.1\" \\\n",
    "--huber_schedule=\"snr\" \\\n",
    "--learning_rate=\"0.0001\" \\\n",
    "--unet_lr=0.0001 \\\n",
    "--text_encoder_lr=5e-05 \\\n",
    "--loss_type=\"l2\" \\\n",
    "--lr_scheduler=\"cosine_with_restarts\" \\\n",
    "--lr_scheduler_num_cycles=\"4\" \\\n",
    "--lr_warmup_steps=\"400\" \\\n",
    "--max_grad_norm=\"1\" \\\n",
    "--mem_eff_attn \\\n",
    "--min_timestep=0 \\\n",
    "--network_alpha=\"128\" \\\n",
    "--network_dim=128 \\\n",
    "--network_module=networks.lora \\\n",
    "--optimizer_type=\"AdamW8bit\" \\\n",
    "--max_train_steps=\"4000\" \\\n",
    "--train_batch_size=\"2\" \\\n",
    "--max_data_loader_n_workers=\"0\" \\\n",
    "--save_every_n_epochs=\"1\" \\\n",
    "--save_model_as=safetensors \\\n",
    "--save_precision=\"fp16\" \\\n",
    "--xformers\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here you can choose to `download the trained LoRA checkpoints` from my Hugging Face repository (by default).**\n",
    "\n",
    "The 4 trained LoRA checkpoints (`.safetensors`) are on my Hugging Face repository (https://huggingface.co/kevin-chu/sd15-lora-geralt-of-rivia), details are as below:\n",
    "- `lora-train-000001.safetensors`: 1 epoch checkpoint\n",
    "- `lora-train-000002.safetensors`: 2 epochs checkpoint\n",
    "- `lora-train-000003.safetensors`: 3 epochs checkpoint\n",
    "- `lora-train.safetensors`: 4 epochs checkpoint (complete training)\n",
    "Please put the checkpoints to this directory `./output/model` for further access.\n",
    "\n",
    "**Notice: If Canvas allows large files to upload, you will see the 4 LoRA checkpoints already exist in this directory `./output/model`, and you do not need to download again for this case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the trained LoRA checkpoints from my Hugging Face repository (skip this part if you want to run the training script)\n",
    "snapshot_download(\n",
    "    repo_id=\"kevin-chu/sd15-lora-geralt-of-rivia\",\n",
    "    allow_patterns=\"*.safetensors\",\n",
    "    local_dir=\"./output/model\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have run the training script, and then reached here, it means your training is complete. You can scroll up and refresh the Tensorboard to see your training results!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Testing Section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-1: Testing hyperparameters and helper functions**\n",
    "Several helper functions are designed for the testing section, it provides a more systematic way to demonstrate the results and compare the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters for the testing\n",
    "steps = [50]\n",
    "scale = 7\n",
    "width = 512\n",
    "height = 512\n",
    "num_images = 1\n",
    "generator = torch.Generator(device=\"cuda\")\n",
    "clip_skip = 2\n",
    "\n",
    "# Create a list of weight names\n",
    "weight_names = [\"lora-train-000001.safetensors\", \"lora-train-000002.safetensors\", \"lora-train-000003.safetensors\", \"lora-train.safetensors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading LoRA adapters to base or pre-trained model\n",
    "def load_lora_adapters(pipe, weight_names):\n",
    "\n",
    "    # Load the trained model with the specified LoRA weights and generate images (starting from lora-1 to lora-4)\n",
    "    for i, weight_name in enumerate(weight_names, 1):\n",
    "\n",
    "        # Load the base model with the specified LoRA weights (each adapter named as lora-1, lora-2, lora-3, and lora-4)\n",
    "        pipe.load_lora_weights(\"kevin-chu/sd15-lora-geralt-of-rivia\", weight_name=weight_name, adapter_name=f\"lora-{i}\")\n",
    "\n",
    "    # Set/Replace adapter names\n",
    "    adapter_names = list(pipe.get_list_adapters().values())\n",
    "    adapter_names = [item for sublist in adapter_names[:len(adapter_names)//2] for item in sublist]\n",
    "    \n",
    "    return adapter_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating images with the specified adapter using different weights\n",
    "def generate_images_with_adapter(pipe, positive_prompt, negative_prompt, seed_num, inference_steps_num, adapter_names, lora_weights, is_adapter_merged=False, added_adaper_name = None, adapters_merged_weights = None):\n",
    "\n",
    "    # Create an empty list for images\n",
    "    images = []\n",
    "\n",
    "    # Iterating through all combinations of steps, adapter names, and LoRA weights\n",
    "    for step in inference_steps_num:\n",
    "        for lora_weight  in lora_weights:\n",
    "            for adapter_name in adapter_names:\n",
    "\n",
    "                # Activate the specified adapter\n",
    "                if is_adapter_merged == True:\n",
    "                    # Merge the adapters with provided LoRA weights\n",
    "                    pipe.set_adapters([added_adaper_name, adapter_name], adapters_merged_weights)\n",
    "                else:\n",
    "                    # Not merge the adapters\n",
    "                    pipe.set_adapters(adapter_name)\n",
    "\n",
    "                # Generate images with LoRA adapters\n",
    "                image = pipe(positive_prompt, negative_prompt=negative_prompt, width=width, height=height, num_inference_steps=step, guidance_scale=scale, clip_skip = clip_skip,\n",
    "                                    num_images_per_prompt=num_images, generator=generator.manual_seed(seed_num), cross_attention_kwargs={\"scale\": lora_weight}\n",
    "                ).images[0]\n",
    "\n",
    "                # Append the generated image to the list\n",
    "                images.append(image)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for image display (LoRA adapters with different weights)\n",
    "def display_images_lora_weights(images, adapter_names, lora_weights):\n",
    "    \n",
    "        # Create a figure with LoRA weights as rows and LoRA adapter names as columns\n",
    "        fig, axs = plt.subplots(len(lora_weights), len(adapter_names), figsize=(20, 5 * len(lora_weights)))\n",
    "\n",
    "        # Set counters for x and y axis labels\n",
    "        x_counter = 0\n",
    "        y_counter = 0\n",
    "\n",
    "        # Display the images\n",
    "        for i in range(len(lora_weights)):\n",
    "            for j in range(len(adapter_names)):\n",
    "    \n",
    "                # Calculate the index of the image\n",
    "                index = i * len(adapter_names) + j\n",
    "                axs[i, j].imshow(images[index])\n",
    "    \n",
    "                # Turn off the axis lines and ticks\n",
    "                axs[i, j].set_xticks([])\n",
    "                axs[i, j].set_yticks([])\n",
    "    \n",
    "                # Set the x,y axis label (as title) for this subplot\n",
    "                if i == 0:  # Only set x-axis labels for the top row\n",
    "                    axs[i, j].set_title(f\"{adapter_names[x_counter]} ({x_counter + 1} epoch)\", fontsize=14, fontweight='bold')\n",
    "                    x_counter += 1\n",
    "    \n",
    "                if j == 0:  # Only set y-axis labels for the first column\n",
    "                    axs[i, j].set_ylabel(f\"lora-weight: {lora_weights[y_counter]}\", fontsize=14, fontweight='bold')\n",
    "                    y_counter += 1\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for image display (different steps)\n",
    "def display_images_steps(images, test_steps, pipe):\n",
    "    \n",
    "        # Create a figure with LoRA adapter name as rows and steps as columns\n",
    "        fig, axs = plt.subplots(len(pipe.get_active_adapters()), len(test_steps), figsize=(20, 5 * len(pipe.get_active_adapters())))\n",
    "\n",
    "        # Set counters for x and y axis labels\n",
    "        x_counter = 0\n",
    "        y_counter = 0\n",
    "\n",
    "        # Display the images\n",
    "        for i in range(len(pipe.get_active_adapters())):\n",
    "            for j in range(len(test_steps)):\n",
    "    \n",
    "                # Calculate the index of the image\n",
    "                index = i * len(test_steps) + j\n",
    "\n",
    "                # Check if the figure is only 1-dimensional\n",
    "                if axs.ndim == 1:\n",
    "                    ax = axs[j]\n",
    "                else:\n",
    "                    ax = axs[i, j]\n",
    "                ax.imshow(images[index])\n",
    "\n",
    "                # Turn off the axis lines and ticks\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "\n",
    "                # Set the x,y axis label (as title) for this subplot\n",
    "                if i == 0:  # Only set x-axis labels for the top row\n",
    "                    ax.set_title(f\"inference steps: {test_steps[x_counter]}\", fontsize=14, fontweight='bold')\n",
    "                    x_counter += 1\n",
    "\n",
    "                if j == 0:  # Only set y-axis labels for the first column\n",
    "                    ax.set_ylabel(f\"{pipe.get_active_adapters()[y_counter]} (lora weight: 1.0)\", fontsize=14, fontweight='bold')\n",
    "                    y_counter += 1\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion for freeing cuda memory (must first delete pipeline then empty cache)\n",
    "def free_cuda_memory():\n",
    "    globals().pop(\"sd_pipeline\", None)\n",
    "    globals().pop(\"test_images\", None)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Show GPU memory status\n",
    "def memory_stats():\n",
    "    print(torch.cuda.memory_allocated()/1024**2)\n",
    "    print(torch.cuda.memory_cached()/1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-2: Test 1 - Comparisons of generated images with the base model and the LoRA-trained adapters**\n",
    "We would like to compare the generated images `before` and `after` applying the LoRA-trained adapters to the base model `Stable Diffusion v1.5`, and see what interesting results we can find! (Reference: `Stable Diffusion v1.5` https://huggingface.co/runwayml/stable-diffusion-v1-5)\n",
    "\n",
    "The testing process is as follows:\n",
    "- Step 1: Load base model\n",
    "- Step 2: Load LoRA adapters to the base model\n",
    "- Step 3: Generate and show the result images\n",
    "- Step 4: Free GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Load base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model (Stable Diffusion v1.5) for comparison (we close the safety checker to avoid detection errors of poor quality images)\n",
    "sd_pipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True, safety_checker=None).to(\"cuda\")\n",
    "\n",
    "# Set the scheduler to DPMSolver\n",
    "sd_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(sd_pipeline.scheduler.config)\n",
    "\n",
    "# Optimize the pipeline for memory-efficient attention\n",
    "sd_pipeline.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
    "sd_pipeline.vae.enable_xformers_memory_efficient_attention(attention_op=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Load LoRA adapters to the base model**\n",
    "We set the `LoRA weights` to `0, 0.25, 0.5, 0.75, 1.0` (minimum is `0` / maximum is `1`), which means how much the LoRA would affect the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of adapter names and LoRA weights\n",
    "lora_names = []\n",
    "lora_weights = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Load LoRA adapters to the base model\n",
    "lora_names = load_lora_adapters(sd_pipeline, weight_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Generate and show the result images**\n",
    "We choose a simple `positive prompt` here with `no negative prompt`, and keep the `seed to 1` to reproduce the same process for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test prompts\n",
    "test_positive_prompt = \"geralt of rivia\"\n",
    "test_negative_prompt = \"\"\n",
    "seed = 1\n",
    "\n",
    "# Generate images with the specified adapter names and LoRA weights\n",
    "test_images = generate_images_with_adapter(sd_pipeline, test_positive_prompt, test_negative_prompt, seed, steps, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "display_images_lora_weights(test_images, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Free GPU memory**\n",
    "- The process of generating images takes up a lot of GPU VRAM memory.\n",
    "- We have already applied `xformers` to the model pipeline, but it is still good to always know free the GPU memory before the next test case (except if we want to reuse the pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free cuda memory\n",
    "free_cuda_memory()\n",
    "\n",
    "# Show current GPU memory status\n",
    "memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-3: Test 2 - Apply the different styles of LoRA adapters on the base model**\n",
    "- We would like to compare the generated images of a `pixel` style LoRA merging with the LoRA adapters we trained, then we apply each merged adapter to the base model `Stable Diffusion v1.5`.\n",
    "- The pixel-style LoRA is from `CivitAI` (https://civitai.com/models/44960/mpixel, file name: `pixel_f2.safetensors`), which can generate decent pixel-style images while using this LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Download the adapter and load LoRA adapters to the base model**\n",
    " - You can download this model using `wget`\n",
    " - If you have not installed `wget` before (most likely on Windows OS), you can choose to download the model directly from the provided `CivitAI` link above.\n",
    " - Remember to download it to the directory of the base-model folder as `./based-model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained LoRA (pixel style)\n",
    "!wget \"https://civitai.com/api/download/models/52870?type=Model&format=SafeTensor\" -O ./based-model/pixel_f2.safetensors\n",
    "\n",
    "# Load base model (Stable Diffusion v1.5) and scheduler\n",
    "sd_pipeline = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True, safety_checker=None).to(\"cuda\")\n",
    "sd_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(sd_pipeline.scheduler.config)\n",
    "\n",
    "# Optimize the pipeline for memory-efficient attention\n",
    "sd_pipeline.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
    "sd_pipeline.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n",
    "\n",
    "# Load LoRA adapters to the base model\n",
    "lora_names = load_lora_adapters(sd_pipeline, weight_names)\n",
    "sd_pipeline.load_lora_weights(\"./based-model\", weight_name=\"pixel_f2.safetensors\", adapter_name=\"pixel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Generate and show the result images**\n",
    "Settings for this test are as follows:\n",
    "- `LoRA weights`: 0, 0.25, 0.5, 0.75, 1.0\n",
    "- `Positive prompt`: geralt of rivia, pixel (`pixel` is the trigger word for the downloaded LoRA)\n",
    "- `Negative prompt`: None\n",
    "- `merged_weights` : [0.5, 0.5] the `former value` is for `the pixel LoRA`, and the `latter value` is for `each of our trained LoRA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test prompts for the pre-trained model\n",
    "test_positive_prompt = \"geralt of rivia, pixel\"\n",
    "test_negative_prompt = \"\"\n",
    "seed = 924608315\n",
    "merged_weights = [0.5, 0.5]\n",
    "added_adaper_name = \"pixel\"\n",
    "\n",
    "# Generate images with the specified adapter names and LoRA weights\n",
    "test_images = generate_images_with_adapter(sd_pipeline, test_positive_prompt, test_negative_prompt, seed, steps, lora_names, lora_weights, True, added_adaper_name, merged_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "display_images_lora_weights(test_images, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Free GPU memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free cuda memory\n",
    "free_cuda_memory()\n",
    "\n",
    "# Show current GPU memory status\n",
    "memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4-4: Test 3 - Apply LoRA adapters on other pre-trained checkpoint models**\n",
    "- Show different cases of style merging with our LoRA adapters and other pre-trained models (different styles).\n",
    "- All `three` style pre-trained models are based on `Stabel Diffusion v1.5`.\n",
    "- The styles are `anime`, `comic`, and `realistic` style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Case 1: LoRA merges with an anime-style pre-trained model**\n",
    "- We would like to compare the generated images of applying our LoRA adapters to the pre-trained model with `anime style`.\n",
    "- This checkpoint model is from `Hugging Face` (https://huggingface.co/gsdf/Counterfeit-V3.0, file name: `Counterfeit-V3.0_fix_fp16.safetensors`).\n",
    "- You can also find it on `CivitAI` (https://civitai.com/models/4468/counterfeit-v30?modelVersionId=57618).\n",
    "- This checkpoint model can generate high-quality anime-style images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1: Download the checkpoint model and load LoRA adapters**\n",
    "If you choose to download manually, you can put it into the base-model folder as this directory `./based-model`, then modify `model_dir` to `./based-model/Counterfeit-V3.0_fix_fp16.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained model (anime style)\n",
    "snapshot_download(\n",
    "    repo_id=\"gsdf/Counterfeit-V3.0\",\n",
    "    allow_patterns=\"Counterfeit-V3.0_fix_fp16.safetensors\",\n",
    "    local_dir=\"./based-model\"\n",
    ")\n",
    "model_dir = \"./based-model/Counterfeit-V3.0_fix_fp16.safetensors\"\n",
    "\n",
    "# Load the pre-trained model and scheduler\n",
    "sd_pipeline = StableDiffusionPipeline.from_single_file(model_dir, torch_dtype=torch.float16).to(\"cuda\")\n",
    "sd_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(sd_pipeline.scheduler.config)\n",
    "sd_pipeline.safety_checker = None\n",
    "\n",
    "# Optimize the pipeline for memory-efficient attention\n",
    "sd_pipeline.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
    "sd_pipeline.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n",
    "\n",
    "# Load LoRA adapters to the base model\n",
    "lora_names = load_lora_adapters(sd_pipeline, weight_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2: Generate and show the result images**\n",
    "Settings for this test are as follows:\n",
    "- `LoRA weights`: 0, 0.25, 0.5, 0.75, 1.0\n",
    "- `Positive prompt`: 1boy, geralt of rivia\n",
    "- `Negative prompt`: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test prompts for the pre-trained model\n",
    "test_positive_prompt = \"1boy, geralt of rivia\"\n",
    "test_negative_prompt = \"\"\n",
    "seed = 629389646\n",
    "\n",
    "# Generate images with the specified adapter names and LoRA weights\n",
    "test_images = generate_images_with_adapter(sd_pipeline, test_positive_prompt, test_negative_prompt, seed, steps, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "display_images_lora_weights(test_images, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3: Free GPU memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free cuda memory\n",
    "free_cuda_memory()\n",
    "\n",
    "# Show current GPU memory status\n",
    "memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Case 2: LoRA merges with a comic-style pre-trained model**\n",
    "- We would like to compare the generated images of applying our LoRA adapters to the pre-trained model with `comic style`.\n",
    "- This checkpoint model is from `CivitAI` (https://civitai.com/models/35960/flat-2d-animerge?modelVersionId=266360, file name: `flat2DAnimerge_v45Sharp.safetensors`).\n",
    "- This checkpoint model can generate high-quality comic-style images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1: Download the checkpoint model and load LoRA adapters**\n",
    "If you choose to download manually, you can put it into the base-model folder as this directory `./based-model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained model (comic style)\n",
    "!wget \"https://civitai.com/api/download/models/266360?type=Model&format=SafeTensor&size=pruned&fp=fp16\" -O ./based-model/flat2DAnimerge_v45Sharp.safetensors\n",
    "model_dir = \"./based-model/flat2DAnimerge_v45Sharp.safetensors\"\n",
    "\n",
    "# Load the pre-trained model and scheduler\n",
    "sd_pipeline = StableDiffusionPipeline.from_single_file(model_dir, torch_dtype=torch.float16).to(\"cuda\")\n",
    "sd_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(sd_pipeline.scheduler.config)\n",
    "sd_pipeline.safety_checker = None\n",
    "\n",
    "# Optimize the pipeline for memory-efficient attention\n",
    "sd_pipeline.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
    "sd_pipeline.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n",
    "\n",
    "# Load LoRA adapters to the base model\n",
    "lora_names = load_lora_adapters(sd_pipeline, weight_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2: Generate and show the result images**\n",
    "Settings for this test are as follows:\n",
    "- `LoRA weights`: 0, 0.25, 0.5, 0.75, 1.0\n",
    "- `Positive prompt`: geralt of rivia, 1boy, masterpiece, best quality\n",
    "- `Negative prompt`: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n",
    "\n",
    "This time we add some `standard` positive and negative prompts for generating images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test prompts for the pre-trained model\n",
    "test_positive_prompt = \"geralt of rivia, 1boy, masterpiece, best quality\"\n",
    "test_negative_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"\n",
    "seed = 3330428146\n",
    "\n",
    "# Generate images with the specified adapter names and LoRA weights\n",
    "test_images = generate_images_with_adapter(sd_pipeline, test_positive_prompt, test_negative_prompt, seed, steps, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "display_images_lora_weights(test_images, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3: Free GPU memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free cuda memory\n",
    "free_cuda_memory()\n",
    "\n",
    "# Show current GPU memory status\n",
    "memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Case 3: LoRA merges with a realistic-style pre-trained model**\n",
    "- We would like to compare the generated images of applying our LoRA adapters to the pre-trained model with `realistic style`.\n",
    "- This checkpoint model is from `CivitAI` (https://civitai.com/models/4201/realistic-vision-v60-b1?modelVersionId=130072, file name: `realisticVisionV60B1_v51VAE.safetensors`).\n",
    "- This checkpoint model can generate high-quality realistic-style images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1: Download the checkpoint model and load LoRA adapters**\n",
    "If you choose to download manually, you can put it into the base-model folder as this directory `./based-model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained model (realistic style)\n",
    "!wget \"https://civitai.com/api/download/models/130072?type=Model&format=SafeTensor&size=pruned&fp=fp16\" -O ./based-model/realisticVisionV60B1_v51VAE.safetensors\n",
    "model_dir = \"./based-model/realisticVisionV60B1_v51VAE.safetensors\"\n",
    "\n",
    "# Load the pre-trained model and scheduler\n",
    "sd_pipeline = StableDiffusionPipeline.from_single_file(model_dir, torch_dtype=torch.float16).to(\"cuda\")\n",
    "sd_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(sd_pipeline.scheduler.config)\n",
    "sd_pipeline.safety_checker = None\n",
    "\n",
    "# Optimize the pipeline for memory-efficient attention\n",
    "sd_pipeline.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
    "sd_pipeline.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n",
    "\n",
    "# Load LoRA adapters to the base model\n",
    "lora_names = load_lora_adapters(sd_pipeline, weight_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2: Generate and show the result images**\n",
    "Settings for this test are as follows:\n",
    "- `LoRA weights`: 0, 0.25, 0.5, 0.75, 1.0\n",
    "- `Positive prompt`: 1boy, geralt of rivia, full body, RAW photo, subject, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\n",
    "- `Negative prompt`: deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers, deformed, distorted, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation\n",
    "\n",
    "This time we add some `complicated` positive and negative prompts which are recommended from the documents of this checkpoint model for generating images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test prompts for the pre-trained model\n",
    "test_positive_prompt = \"1boy, geralt of rivia, full body, RAW photo, subject, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3\"\n",
    "test_negative_prompt = \"deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers, deformed, distorted, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation\"\n",
    "seed = 3599032598\n",
    "steps = [100]\n",
    "\n",
    "# Generate images with the specified adapter names and LoRA weights\n",
    "test_images = generate_images_with_adapter(sd_pipeline, test_positive_prompt, test_negative_prompt, seed, steps, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3: Free GPU memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "display_images_lora_weights(test_images, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Case 4: Generate images with different inference steps for denoising**\n",
    " - We found out `how many inference steps` are crucial for generating good images, while `low inference steps` would result in generated images with massive noise.\n",
    " - High inference steps mean more GPU computing resources and time are required, so it is an important topic for every task to decide the trade-off.\n",
    "\n",
    "We want to try the `realistic` style checkpoint model since it seems to require more `inference steps` than other styles in Cases 1 and 2, it's most likely because the high-resolution images are more demanding for their high quality, and this depends on what kind of visual effect is the style pursuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1: Generate and show the result images**\n",
    "- We `reuse` the settings of the pre-trained model (realistic style), so this time we do not need to download any new checkpoints.\n",
    "- We select `lora-4` which is the adapter trained with the `highest epochs`, and set the `LoRA weight` to its maximum value `1` to challenge the minimum `inference steps` we need to generate a high-quality image without noise residue.\n",
    "\n",
    "Settings for this test are as follows:\n",
    "- `LoRA name`: lora-4 (trained with complete 4 epochs)\n",
    "- `LoRA weight`: 1.0\n",
    "- `inference steps`: 30, 40, 50, 60, 70\n",
    "- `Positive prompt`: geralt of rivia, 1boy, masterpiece, best quality\n",
    "- `Negative prompt`: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose the LoRA adapter with the longest training period (lora-4 adapter), set the weight to maximum (1.0), and only test on different steps\n",
    "test_steps = [30, 40, 50, 60, 70]\n",
    "lora_names = [\"lora-4\"]\n",
    "lora_weights = [1.0]\n",
    "\n",
    "# Generate images with the specified adapter names and LoRA weights\n",
    "test_images = generate_images_with_adapter(sd_pipeline, test_positive_prompt, test_negative_prompt, seed, test_steps, lora_names, lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the images\n",
    "display_images_steps(test_images, test_steps, sd_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2: Free GPU memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free cuda memory\n",
    "free_cuda_memory()\n",
    "\n",
    "# Show current GPU memory status\n",
    "memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ending: Special Thanks**\n",
    "\n",
    "<p style=\"font-size:24px;\"> Thank you for reading this project, this comes to the end of the project based on the course EECE 570 - Fundamentals of Visual Computing (2023 Winter Session Term 2) at the University of British Columbia, Canada. </p>\n",
    "\n",
    "<p style=\"font-size:24px;\"> I extend my deepest gratitude to Professor Xiaoxiao Li and all the teaching assistants who have enriched this course with their deep knowledge of the latest research and developments in Computer Vision. </p>\n",
    "\n",
    "<p style=\"font-size:24px;\"> As AI/ML technologies improve at an incredibly fast pace, their potential to significantly change our lives becomes more evident each day. This project serves as a great start for further exploration of the Computer Vision and Generative AI fields. I am excited about the potential directions of this research project and looking forward to expanding this project to include future topics with great value. </p>\n",
    "\n",
    "<p style=\"font-size:24px;\"> Special thanks to the teams at Stability AI and Hugging Face, the authors of open-source projects (Kohya, webUI, etc.), and all the researchers and developers contributing to the ML field. I am positive to see a bright future with innovations that technology will bring to us. </p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
